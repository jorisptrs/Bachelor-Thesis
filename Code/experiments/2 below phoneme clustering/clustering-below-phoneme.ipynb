{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74e86d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Failed\n",
      "Collecting Features from Audio Files\n",
      "1000\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "--- Completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "import importlib\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('c:\\\\main\\\\Work\\\\thesis\\\\Bachelor-Thesis\\\\Code\\\\hierarchical-clustering\\\\hierarchical_clustering.py'))))\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import gc\n",
    "import numpy as np\n",
    "from treelib import Node, Tree\n",
    "from timit.loading import Feature_Collector\n",
    "from lib.conceptors import *\n",
    "from lib.esn import ESN\n",
    "from lib.helpers import *\n",
    "from lib.plot import Plot\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "###-------------- Collecting Training Features -------------------\n",
    "path = '../timit/'\n",
    "fc = Feature_Collector(path)\n",
    "\n",
    "save = True\n",
    "\n",
    "dr = []\n",
    "speakers = []\n",
    "long_version = False\n",
    "n_mels = 10\n",
    "delta = False\n",
    "delta_delta = False\n",
    "subsamples = 4\n",
    "fc.readTrainingDataDescriptionCSV()\n",
    "speakers = fc._Tdd.speaker_id.unique()\n",
    "speakers = np.random.choice(speakers,100,replace=False)\n",
    "sentence = \"SX\"\n",
    "\n",
    "path_option = str(long_version)+\"_\"+str(n_mels)+\"_\"+str(delta)+\"_\"+str(delta_delta)+\"_\"+str(subsamples)\n",
    "\n",
    "if dr:\n",
    "    path_option = str(dr)+\"_\"+path_option\n",
    "if len(speakers):\n",
    "    path_option = str(speakers[0])+\"_\"+path_option\n",
    "\n",
    "features,labels,oversamplings = fc.collectFeaturesInSegments(\n",
    "    n_mels=n_mels,delta=delta,delta_delta=delta_delta,\n",
    "    long_version=long_version,speakers=speakers,dr=dr,\n",
    "    subsamples=subsamples,path_option=path_option)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a08349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init reservoir\n",
    "import random\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "t_washout = 0 # number of washout steps\n",
    "aperture = 5\n",
    "nb_trials = 1\n",
    "\n",
    "esn_params = {\n",
    "    \"in_dim\": n_mels*(1+delta+delta_delta),\n",
    "    \"out_dim\": n_mels*(1+delta+delta_delta),\n",
    "    \"N\": 20,\n",
    "    \"W_in_scale\": 1.5,\n",
    "    \"b_scale\": .2,\n",
    "    \"spectral_radius\": 1.5\n",
    "}\n",
    "\n",
    "esn = ESN(esn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d56f220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "2985\n",
      "7 phonemes\n",
      "['ih', 'aa', 'iy', 'eh', 'ah', 'uh', 'ae']\n"
     ]
    }
   ],
   "source": [
    "group = {}\n",
    "selected_labels = [\"aa\", \"ae\", \"ah\", \"eh\", \"ih\", \"iy\", \"uh\"]\n",
    "# { phoneme : samples }, where the samples = [ [x[0],x[1]...], [] ]\n",
    "for i in range(len(features)):\n",
    "    if labels[i] not in selected_labels:\n",
    "        continue\n",
    "    if labels[i] not in group.keys():\n",
    "        group[labels[i]] = []\n",
    "    group[labels[i]].append(features[i])\n",
    "print(len(group.keys()))\n",
    "min_samples = max([len(samples) for samples in group.values()])\n",
    "\n",
    "phonemes = list(group.keys())\n",
    "print(min_samples)\n",
    "print(str(len(phonemes))+\" phonemes\")\n",
    "print(phonemes)\n",
    "#[print(x,\" ss \",len(y)) for x,y in group.items() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d51007d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- computing conceptors\n",
      "ih  -  2985  samples\n",
      "aa  -  1265  samples\n",
      "iy  -  1470  samples\n",
      "eh  -  829  samples\n",
      "ah  -  1354  samples\n",
      "uh  -  113  samples\n",
      "ae  -  823  samples\n",
      "- computing negative conceptors\n",
      "- optimizing +\n",
      "- optimizing -\n",
      "--- Done\n"
     ]
    }
   ],
   "source": [
    "save = False\n",
    "params = str(aperture)+\"_\"+str(esn_params[\"N\"])+\"_\"+str(min_samples)\n",
    "\n",
    "file_name = \"_\"+params+\"_\"+path_option+\"_\"+str(selected_labels)\n",
    "\n",
    "if save and os.path.exists('./cache/working/'+\"Cs\"+file_name+'.pkl') and os.path.exists('./cache/working/'+\"Ns\"+file_name+'.pkl'):\n",
    "    print(\"- loading conceptors from file\")\n",
    "    Cs_fp = open('./cache/working/'+\"Cs\"+file_name+'.pkl','rb')\n",
    "    Cs = pkl.load(Cs_fp)\n",
    "    Cs_fp.close()\n",
    "    Ns_fp = open('./cache/working/'+\"Ns\"+file_name+'.pkl','rb')\n",
    "    Ns = pkl.load(Ns_fp)\n",
    "    Ns_fp.close()\n",
    "    print(\"--- Done\")\n",
    "else:\n",
    "    print(\"- computing conceptors\")\n",
    "    Cs = []\n",
    "    Cs_labels = []\n",
    "    all_signals = []\n",
    "    for phoneme, signals in group.items():\n",
    "        print(phoneme,\" - \", len(signals), \" samples\")\n",
    "        for signal in signals:\n",
    "            x,_ = esn.run_X(signal.T,0,signal.shape[0])\n",
    "            Cs.append(compute_c(x, aperture))\n",
    "            Cs_labels.append(phoneme)\n",
    "            all_signals.append(signal)\n",
    "    print(\"- computing negative conceptors\")\n",
    "    #Ns = Ns_from_Cs(Cs)\n",
    "    print(\"- optimizing +\")\n",
    "    Cs = optimize_apertures(Cs)\n",
    "    print(\"- optimizing -\")    \n",
    "    #Ns = optimize_apertures(Ns)\n",
    "    \n",
    "    if save:\n",
    "        Cs_fp = open(\"./cache/working/\"+\"Cs\"+file_name+\".pkl\",'wb')\n",
    "        pkl.dump(Cs,Cs_fp)\n",
    "        Cs_fp.close()\n",
    "        Ns_fp = open(\"./cache/working/\"+\"Ns\"+file_name+\".pkl\",'wb')\n",
    "        pkl.dump(Ns,Ns_fp)\n",
    "        Ns_fp.close()\n",
    "    \n",
    "    print(\"--- Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6355d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------ Custom test\n",
    "correct = 0\n",
    "n=0\n",
    "fv, lv, _ = fc.collectFeaturesInSegments(\n",
    "    ft='Test',n_mels=n_mels,delta=delta,delta_delta=delta_delta,\n",
    "    long_version=long_version,speakers=speakers,dr=dr,subsamples=subsamples,\n",
    "    path_option=path_option+\"_test\")\n",
    "\n",
    "test_group = {}\n",
    "for key in group.keys():\n",
    "    test_group[key] = []\n",
    "\n",
    "for i in range(len(fv)):\n",
    "    if lv[i] in group.keys():\n",
    "        test_group[lv[i]].append(fv[i])\n",
    "\n",
    "for i,y in enumerate(test_group.items()):\n",
    "    for sample in y[:200]:\n",
    "        z = esn.run(sample.T)\n",
    "        es = evidences_for_Cs_z(z,Cs,Ns)\n",
    "        es = [ np.sum(p) for p in es ]\n",
    "        correct += x==list(test_group.keys())[np.argmax(es)]\n",
    "        n+=1\n",
    "        #print(list(group.keys())[np.argmax(es)])\n",
    "print(\"Accuracy: \",correct/n)\n",
    "print(\"n_mels: \",n_mels)\n",
    "print(\"d: \",delta)\n",
    "print(\"dd: \",delta_delta)\n",
    "print(\"speakers: \",speakers)\n",
    "print(\"dr: \",dr)\n",
    "print(\"Min_samples: \",min_samples)\n",
    "print(\"Test: \",\"Test\")\n",
    "print(\"subsamples: \",subsamples)\n",
    "print(\"Neurons: \",esn_params[\"N\"])\n",
    "print(\"Aperture: \",aperture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9432642e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- from file\n",
      "--- Done\n"
     ]
    }
   ],
   "source": [
    "save = True\n",
    "dist_fun = \"cos\"\n",
    "hm_filename = \"_5_40_1100_False_14_False_False_10_hm_cos\"\n",
    "\n",
    "if save and os.path.exists('./cache/working/'+hm_filename+'.pkl'):\n",
    "    print(\"- from file\")\n",
    "    fp = open('./cache/working/'+hm_filename+'.pkl','rb')\n",
    "    heat_map = pkl.load(fp)\n",
    "    fp.close()\n",
    "    print(\"--- Done\")\n",
    "else:\n",
    "    print(\"- computing heatmap\")\n",
    "    heat_map = np.zeros((len(phonemes),len(phonemes)))\n",
    "\n",
    "    for x in range(len(phonemes)):\n",
    "        for y in range(0, x+1):\n",
    "            sim = similarity_c(Cs[x], Cs[y])\n",
    "            heat_map[x,y] = sim\n",
    "            heat_map[y,x] = sim    \n",
    "    if save:\n",
    "        fp = open(\"./cache/working/\"+hm_filename+\".pkl\",'wb')\n",
    "        pkl.dump(heat_map,fp)\n",
    "        fp.close()\n",
    "    print(\"--- Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7229bf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "coll = []\n",
    "hm = heat_map.copy()\n",
    "\n",
    "for t in range(len(phonemes)**2):\n",
    "    max = 0\n",
    "    maxx = 0\n",
    "    maxy = 0\n",
    "    for x in range(hm.shape[0]):\n",
    "        for y in range(0, x):\n",
    "            if max < hm[x,y]:\n",
    "                max = hm[x,y]\n",
    "                maxx = x\n",
    "                maxy = y\n",
    "    coll.append((phonemes[maxx],phonemes[maxy],hm[maxx,maxy]))\n",
    "    hm[maxx, maxy] = 0\n",
    "\n",
    "for x,y,val in coll:\n",
    "    print(x,\" \",y,\" \",val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32c154e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions of Z and labels must be consistent.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDendrograms\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Create dendrogram\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdendrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mphonemes\u001b[49m\u001b[43m,\u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mright\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdistance_sort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#plt.title(\"Average Linkage on Phoneme Conceptors using Herbert's Distance Metric\",fontsize=15)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPhoneme\u001b[39m\u001b[38;5;124m'\u001b[39m,fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\scipy\\cluster\\hierarchy.py:3299\u001b[0m, in \u001b[0;36mdendrogram\u001b[1;34m(Z, p, truncate_mode, color_threshold, get_leaves, orientation, labels, count_sort, distance_sort, show_leaf_counts, no_plot, no_labels, leaf_font_size, leaf_rotation, leaf_label_func, show_contracted, link_color_func, ax, above_threshold_color)\u001b[0m\n\u001b[0;32m   3295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation must be one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3296\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbottom\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m Z\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels):\n\u001b[1;32m-> 3299\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimensions of Z and labels must be consistent.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3301\u001b[0m is_valid_linkage(Z, throw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   3302\u001b[0m Zs \u001b[38;5;241m=\u001b[39m Z\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mValueError\u001b[0m: Dimensions of Z and labels must be consistent."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy.cluster.hierarchy as sc\n",
    "import scipy.spatial.distance as ssd\n",
    "\n",
    "for i in range(heat_map.shape[0]):\n",
    "    heat_map[i,i] = 0\n",
    "distArray = ssd.squareform(heat_map)\n",
    "\n",
    "if dist_fun == 'cos':\n",
    "    distances = 1-distArray/np.max(distArray)\n",
    "elif dist_fun == 'eucl':\n",
    "    distances = distArray/np.max(distArray)\n",
    "\n",
    "link = sc.linkage(distances, method='average', optimal_ordering=False)\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(dpi=120)\n",
    "plt.title(\"Dendrograms\")  \n",
    "# Create dendrogram\n",
    "sc.dendrogram(link,labels=phonemes,orientation='right',distance_sort=False)\n",
    "\n",
    "#plt.title(\"Average Linkage on Phoneme Conceptors using Herbert's Distance Metric\",fontsize=15)\n",
    "plt.ylabel('Phoneme',fontsize=15)\n",
    "plt.xlabel(r\"Distance d$(p^1, p^2)=1-\\frac{sim(C^1,C^2)}{max_{i,j}sim(C^i,C^j)}$\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78d0ae15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h#', 'w', 'ih', 's', 'ah', 'ch', 'n', 'ae', 't', 'v', 'r', 'f', 'y', 'uw', 'sh', 'l', 'b', 'iy', 'aa', 'd', 'eh', 'p', 'z', 'ey', 'dx', 'ay', 'ng', 'k', 'dh', 'er', 'm', 'jh', 'g', 'ow', 'aw', 'hh', 'uh', 'oy', 'th']\n",
      "34 39\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 35 is out of bounds for axis 0 with size 34",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(labs),\u001b[38;5;28mlen\u001b[39m(phonemes))\n\u001b[0;32m     16\u001b[0m reorder \u001b[38;5;241m=\u001b[39m [ phonemes\u001b[38;5;241m.\u001b[39mindex(lab) \u001b[38;5;28;01mfor\u001b[39;00m lab \u001b[38;5;129;01min\u001b[39;00m labs]\n\u001b[1;32m---> 17\u001b[0m hm \u001b[38;5;241m=\u001b[39m \u001b[43mheat_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mreorder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     18\u001b[0m hm \u001b[38;5;241m=\u001b[39m hm[:,reorder]\n\u001b[0;32m     19\u001b[0m ax \u001b[38;5;241m=\u001b[39m sns\u001b[38;5;241m.\u001b[39mheatmap(hm, xticklabels\u001b[38;5;241m=\u001b[39mlabs, yticklabels\u001b[38;5;241m=\u001b[39mlabs, linewidths\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.5\u001b[39m, center\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(heat_map)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m.06\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 35 is out of bounds for axis 0 with size 34"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3400x3000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as patches\n",
    "plt.figure(figsize=(17,15),dpi=200)\n",
    "sns.set_theme()\n",
    "\n",
    "ticklabels=phonemes\n",
    "print(phonemes)\n",
    "sns.set(font_scale=2)\n",
    "labs = ['dh','v',\"h#\",'k','t','p','hh','g','d','b','dx','ah','ih','uw','ey','iy','eh','ae','ay','ow','aa','er','r','l','w','ng','n','m','y','z','s','sh','jh','f']\n",
    "labs = ['g','d',\"b\",'dx','hh','dh','hh','g','d','b','dx','ah','ih','uw','ey','iy','eh','ae','ay','ow','aa','er','r','l','w','ng','n','m','y','z','s','sh','jh','f']\n",
    "\n",
    "print(len(labs),len(phonemes))\n",
    "reorder = [ phonemes.index(lab) for lab in labs]\n",
    "hm = heat_map[reorder,:]\n",
    "hm = hm[:,reorder]\n",
    "ax = sns.heatmap(hm, xticklabels=labs, yticklabels=labs, linewidths=.5, center=np.mean(heat_map)-.06)\n",
    "\n",
    "plt.rc('xtick', labelsize='20')    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize='20')\n",
    "\n",
    "recs = [\n",
    "    [1,1]\n",
    "    \n",
    "]\n",
    "\n",
    "for rec in recs:\n",
    "    ax.add_patch(\n",
    "         patches.Rectangle(\n",
    "             (rec[0], rec[0]),\n",
    "             rec[1],\n",
    "             rec[1],\n",
    "             edgecolor='blue',\n",
    "             fill=False,\n",
    "             lw=5\n",
    "         ) )\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6948756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from math import floor\n",
    "from matplotlib import gridspec\n",
    "\n",
    "\"\"\"\n",
    "Problem-specific plotting\n",
    "\"\"\"\n",
    "class Plot:\n",
    "    def __init__(self, x, y):\n",
    "        plt.rcParams[\"figure.autolayout\"] = True\n",
    "        self.fig = plt.figure(figsize=(y,x))\n",
    "        self.cnt = 0\n",
    "        self.new_ax = None\n",
    "\n",
    "    def add(self, y, label=None):\n",
    "        if label != None:\n",
    "            self.new_ax.plot(y, label=label)\n",
    "            self.new_ax.legend(loc='center left', bbox_to_anchor=(1, 0.5),\n",
    "                                 ncol = 2, fancybox=True, shadow=True,\n",
    "                                 handleheight=2.4, labelspacing=0.05)\n",
    "        else:\n",
    "            self.new_ax.plot(y)\n",
    "\n",
    "    def inc(self):\n",
    "        \"\"\"Plots the data to a new subplot at the bottom.\"\"\"\n",
    "        self.cnt += 1\n",
    "        gs = gridspec.GridSpec(self.cnt, 1)\n",
    "\n",
    "        # Reposition existing subplots\n",
    "        for i, ax in enumerate(self.fig.axes):\n",
    "            ax.set_position(gs[i].get_position(self.fig))\n",
    "            ax.set_subplotspec(gs[i])\n",
    "\n",
    "        # Add new subplot\n",
    "        self.new_ax = self.fig.add_subplot(gs[self.cnt-1])\n",
    "        \n",
    "        \n",
    "    def add_new(self, y, label=\"No Label\"):\n",
    "        self.inc()\n",
    "        self.add(y, label)\n",
    "\n",
    "    def add_new_assignment_plot(self, assignments, labels=[], fuzzy=False, length=0):\n",
    "        self.inc()\n",
    "        for idx, ts in enumerate(assignments):\n",
    "            if not fuzzy:\n",
    "                y = []\n",
    "                if length == 0:\n",
    "                    length = max( [ max(ts) for ts in assignments if ts != [] ] )\n",
    "                for t in range(length):\n",
    "                    if t in ts:\n",
    "                        y.append(1)\n",
    "                    else:\n",
    "                        y.append(0)\n",
    "            else:\n",
    "                y = ts\n",
    "            if max(y) > 0:\n",
    "                if labels == None:\n",
    "                    self.add(smoothed(y,1))\n",
    "                elif labels == []:\n",
    "                    self.add(smoothed(y, 1), idx)\n",
    "                else:\n",
    "                    self.add(smoothed(y, 1), str(labels[idx]))\n",
    "\n",
    "\n",
    "    def add_new_conceptors_fit_plot(self, X, Cs, Ns=None, label=\"\", labels=\"\"):\n",
    "        \"\"\"\n",
    "        Plots, for each time step t, how well each conceptor in Cs matches the state x(t)\n",
    "        \"\"\"\n",
    "        self.inc()\n",
    "        if Ns:\n",
    "            collection = evidences_for_Cs(X,Cs,Ns)\n",
    "        else:\n",
    "            collection, _ = test(X, Cs, \"PROP\")\n",
    "        if labels:\n",
    "            for vals, label in zip(collection, labels):\n",
    "                # walking average of d\n",
    "                self.add(smoothed(vals, 3), label=label)\n",
    "        else:                \n",
    "            for i, vals in enumerate(collection):\n",
    "                # walking average of d\n",
    "                self.add(smoothed(vals, 3), label=label+str(i))\n",
    "\n",
    "\n",
    "    def finalize(self, title=\"\"):\n",
    "        self.fig.suptitle(title, fontsize=16)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2945a718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8981\n"
     ]
    }
   ],
   "source": [
    "all_signals = []\n",
    "for phoneme, signals in group.items():\n",
    "    for signal in signals:\n",
    "        all_signals.append(signal)\n",
    "print(len(all_signals))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8937d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Kmeans helpers\n",
    "\n",
    "def assign_to_clusters_fixed(nb_points, nb_clusters):\n",
    "    \"\"\"\n",
    "    Assigns points to conceptors according to one of several assignment methods\n",
    "    Returns [[points in cluster 1], [points in cluster 2], ...]\n",
    "    \"\"\"\n",
    "    assignments = [ [] for _ in range(nb_clusters) ]\n",
    "    points = [ x for x in range(nb_points) ]\n",
    "    np.random.shuffle(points)\n",
    "    for i in range(nb_clusters):\n",
    "        assignments[i] = points[i*int(nb_points/nb_clusters):(i+1)*int(nb_points/nb_clusters)]\n",
    "    return assignments\n",
    "\n",
    "\n",
    "def F(assignments, correct_assignments):\n",
    "    overlaps = np.zeros((len(assignments), len(correct_assignments))) # (Cs x phonemes)\n",
    "\n",
    "    for i1, assignment in enumerate(assignments):\n",
    "        for a in assignment:\n",
    "            for i2, correct_assignment in enumerate(correct_assignments):\n",
    "                if a in correct_assignment:\n",
    "                    overlaps[i1, i2] += 1\n",
    "\n",
    "    dic = []\n",
    "    cnt = 0\n",
    "    for _ in range(len(correct_assignments)):\n",
    "        i1, i2 = np.unravel_index(overlaps.argmax(), overlaps.shape)\n",
    "        dic.append(i1)\n",
    "        cnt += overlaps[i1,i2]\n",
    "        overlaps[i1,:] = np.zeros((1,len(correct_assignments)))\n",
    "        overlaps[:,i2] = np.zeros((len(assignments),))\n",
    "\n",
    "    return cnt/max([ max(ass) for ass in assignments if ass != [] ])\n",
    "\n",
    "\n",
    "idx=0\n",
    "correct_assignments = [ [] for _ in group.values() ]\n",
    "for i,vals in enumerate(group.values()):\n",
    "    for val in vals:\n",
    "        correct_assignments[i].append(idx)\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1211a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Kmeans\n",
    "# with Evidences\n",
    "####################\n",
    "\n",
    "\n",
    "def k_means_fixed(features, nb_clusters, max_epochs, new_assignments=None):\n",
    "    nb_points = len(features)\n",
    "    if new_assignments == None:\n",
    "        new_assignments = assign_to_clusters_fixed(nb_points, nb_clusters)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        print(\"epoch:\",epoch)\n",
    "        Cs_kmeans = []\n",
    "        for assignments in new_assignments:\n",
    "            if assignments != []:\n",
    "                X = np.array([])\n",
    "                for a in assignments:\n",
    "                    x,_ = esn.run_X(all_signals[a].T,0,all_signals[a].shape[0])\n",
    "                    X = np.hstack((X, x)) if X.size else x\n",
    "                Cs_kmeans.append(compute_c(X, aperture))\n",
    "        Ns_kmeans = Ns_from_Cs(Cs) \n",
    "        Cs = optimize_apertures(Cs)\n",
    "        Ns = optimize_apertures(Ns)\n",
    "        old_assignments = new_assignments.copy()\n",
    "        new_assignments = [ [] for _ in range(len(Cs_kmeans)) ]\n",
    "        for p in range(nb_points):\n",
    "            x,_ = esn.run_X(all_signals[p].T,0,all_signals[p].shape[0])\n",
    "            es = evidences_for_Cs(x,Cs,Ns)\n",
    "            es = [ np.sum(p) for p in es ]\n",
    "            conceptor_index = np.argmax(es)\n",
    "            new_assignments[ conceptor_index ].append(p)\n",
    "        \n",
    "        for new_assignment in new_assignments:\n",
    "            stop = False\n",
    "            for old_assignment in old_assignments:\n",
    "                if set(new_assignment) == set(old_assignment):\n",
    "                    stop = True\n",
    "            if stop:\n",
    "                print(\"Converged\")\n",
    "                return Cs, new_assignments\n",
    "    return Cs_kmeans, new_assignments\n",
    "\n",
    "Cs_kmeans, assignments_kmeans = k_means_fixed(Cs,len(phonemes),15)\n",
    "\n",
    "print(F(assignments_kmeans, correct_assignments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "388d1933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "7\n",
      "epoch: 1\n",
      "4\n",
      "epoch: 2\n",
      "4\n",
      "epoch: 3\n",
      "4\n",
      "epoch: 4\n",
      "4\n",
      "epoch: 5\n",
      "4\n",
      "epoch: 6\n",
      "4\n",
      "epoch: 7\n",
      "4\n",
      "epoch: 8\n",
      "4\n",
      "epoch: 9\n",
      "4\n",
      "epoch: 10\n",
      "4\n",
      "epoch: 11\n",
      "4\n",
      "epoch: 12\n",
      "4\n",
      "epoch: 13\n",
      "4\n",
      "epoch: 14\n",
      "4\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 81>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cnt\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mmax\u001b[39m([ \u001b[38;5;28mmax\u001b[39m(ass) \u001b[38;5;28;01mfor\u001b[39;00m ass \u001b[38;5;129;01min\u001b[39;00m assignments \u001b[38;5;28;01mif\u001b[39;00m ass \u001b[38;5;241m!=\u001b[39m [] ])\n\u001b[0;32m     79\u001b[0m correct_assignments \u001b[38;5;241m=\u001b[39m [ value \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m  group\u001b[38;5;241m.\u001b[39mvalues() ]\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mF\u001b[49m\u001b[43m(\u001b[49m\u001b[43massignments_kmeans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrect_assignments\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36mF\u001b[1;34m(assignments, correct_assignments)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m assignment:\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i2, correct_assignment \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(correct_assignments):\n\u001b[1;32m---> 65\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcorrect_assignment\u001b[49m:\n\u001b[0;32m     66\u001b[0m                 overlaps[i1, i2] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     68\u001b[0m dic \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# Kmeans\n",
    "# Disjunction and similarities\n",
    "####################\n",
    "\n",
    "\n",
    "def k_means_fixed(features, nb_clusters, max_epochs, new_assignments=None):\n",
    "    nb_points = len(features)\n",
    "    if new_assignments == None:\n",
    "        new_assignments = assign_to_clusters_fixed(nb_points, nb_clusters)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        print(\"epoch:\",epoch)\n",
    "        Cs_kmeans = []\n",
    "        for assignments in new_assignments:\n",
    "            if assignments != []:\n",
    "                C = features[assignments[0]]\n",
    "                for a in assignments:\n",
    "                    C = OR_C( C, features[a] )\n",
    "                Cs_kmeans.append(C)\n",
    "        old_assignments = new_assignments.copy()\n",
    "        new_assignments = [ [] for _ in range(len(Cs_kmeans)) ]\n",
    "        for p in range(nb_points):\n",
    "            similarities = [ similarity_c(C,features[p]) for C in Cs_kmeans ]\n",
    "            conceptor_index = np.argmax(similarities)\n",
    "            new_assignments[ conceptor_index ].append(p)\n",
    "        \n",
    "        for new_assignment in new_assignments:\n",
    "            stop = False\n",
    "            for old_assignment in old_assignments:\n",
    "                if set(new_assignment) == set(old_assignment):\n",
    "                    stop = True\n",
    "            if stop:\n",
    "                print(\"Converged\")\n",
    "                return Cs, new_assignments\n",
    "    return Cs_kmeans, new_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c22407",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Kmeans\n",
    "# Disjunction and abstraction\n",
    "####################\n",
    "\n",
    "\n",
    "def k_means_fixed(features, nb_clusters, max_epochs, new_assignments=None):\n",
    "    nb_points = len(features)\n",
    "    if new_assignments == None:\n",
    "        new_assignments = assign_to_clusters_fixed(nb_points, nb_clusters)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        print(\"epoch:\",epoch)\n",
    "        Cs_kmeans = []\n",
    "        for assignments in new_assignments:\n",
    "            if assignments != []:\n",
    "                C = features[assignments[0]]\n",
    "                for a in assignments:\n",
    "                    C = OR_C( C, features[a] )\n",
    "                Cs_kmeans.append(C)\n",
    "        old_assignments = new_assignments.copy()\n",
    "        new_assignments = [ [] for _ in range(len(Cs_kmeans)) ]\n",
    "        for p in range(nb_points):\n",
    "            similarities = [ similarity_c(C,features[p]) for C in Cs_kmeans ]\n",
    "            conceptor_index = np.argmax(similarities)\n",
    "            new_assignments[ conceptor_index ].append(p)\n",
    "        \n",
    "        for new_assignment in new_assignments:\n",
    "            stop = False\n",
    "            for old_assignment in old_assignments:\n",
    "                if set(new_assignment) == set(old_assignment):\n",
    "                    stop = True\n",
    "            if stop:\n",
    "                print(\"Converged\")\n",
    "                return Cs, new_assignments\n",
    "    return Cs_kmeans, new_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "289eef02",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'assignments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 68>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(temp)\n\u001b[0;32m     66\u001b[0m feats, labs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mtemp)\n\u001b[1;32m---> 68\u001b[0m Cs_kmeans, Ns_kmeans, assignments \u001b[38;5;241m=\u001b[39m k_means(feats,labs,\u001b[38;5;28mlen\u001b[39m(phonemes),max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m,new_assignments\u001b[38;5;241m=\u001b[39m\u001b[43massignments\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'assignments' is not defined"
     ]
    }
   ],
   "source": [
    "def assign_to_clusters(nb_points, nb_clusters):\n",
    "    \"\"\"\n",
    "    Assigns points to conceptors according to one of several assignment methods\n",
    "    Returns [[points in cluster 1], [points in cluster 2], ...]\n",
    "    \"\"\"\n",
    "    assignments = [ [] for _ in range(nb_clusters) ]\n",
    "    points = [ x for x in range(nb_points) ]\n",
    "    np.random.shuffle(points)\n",
    "    for i in range(nb_clusters):\n",
    "        assignments[i] = points[i*int(nb_points/nb_clusters):(i+1)*int(nb_points/nb_clusters)]\n",
    "    return assignments\n",
    "\n",
    "def k_means(features, labels, nb_conceptors, max_epochs, new_assignments=None):\n",
    "    \"\"\"\n",
    "    Kmeans algorithm, adapted to conceptors\n",
    "    \"\"\"\n",
    "    # Initial assignments and initial conceptors\n",
    "    nb_points = len(features)\n",
    "    if new_assignments == None:\n",
    "        new_assignments = assign_to_clusters(nb_points, nb_conceptors)\n",
    "    # Training loop\n",
    "    for epoch in range(max_epochs):\n",
    "        print(\"epoch:\",epoch)\n",
    "        # recompute centroids based on subset of assigned state\n",
    "        print(\"- computing conceptors\")\n",
    "        Cs = []\n",
    "        for assignments in new_assignments:\n",
    "            Z = np.array([])\n",
    "            for assignment in assignments:\n",
    "                z = esn.run(features[assignment].T)\n",
    "                Z = np.hstack([Z, z]) if Z.size else z\n",
    "            Cs.append(compute_c(Z, aperture))\n",
    "        print(\"- computing negative conceptors\")\n",
    "        Ns = Ns_from_Cs(Cs)\n",
    "        print(\"- optimizing +\")\n",
    "        Cs = optimize_apertures(Cs)\n",
    "        print(\"- optimizing -\")    \n",
    "        Ns = optimize_apertures(Ns)\n",
    "\n",
    "        # recompute assignments by find the closest conceptor for each of the state points\n",
    "        old_assignments = new_assignments.copy()\n",
    "        new_assignments = [ [] for _ in range(nb_conceptors) ]\n",
    "        for p in range(nb_points):\n",
    "            z = esn.run(features[p].T)    \n",
    "            conceptor_index = np.argmax(np.array(evidences_for_Cs_z(z,Cs,Ns)))\n",
    "            new_assignments[ conceptor_index ].append(p)\n",
    "\n",
    "        # stop if converged\n",
    "        for new_assignment, old_assignment in zip(new_assignments, old_assignments):\n",
    "            if set(new_assignment) == set(old_assignment):\n",
    "                print(\"Converged\")\n",
    "                return Cs, Ns, new_assignments\n",
    "    return Cs, Ns, new_assignments\n",
    "\n",
    "\n",
    "plot = Plot()\n",
    "\n",
    "feats = []\n",
    "labs = []\n",
    "\n",
    "for key, val in group.items():\n",
    "    feats += val\n",
    "    labs += [key for _ in range(len(val))]\n",
    "temp = list(zip(feats, labs))\n",
    "random.shuffle(temp)\n",
    "feats, labs = zip(*temp)\n",
    "\n",
    "Cs_kmeans, Ns_kmeans, assignments = k_means(feats,labs,len(phonemes),max_epochs=15,new_assignments=assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf3ef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_Cs = np.zeros((len(phonemes),len(phonemes))) # Cs x Cs_kmeans\n",
    "\n",
    "matches = 0\n",
    "for x in range(len(phonemes)):\n",
    "    for y in range(len(phonemes)):\n",
    "        sim = similarity_c(Cs[x], Cs_kmeans[y])\n",
    "        sims_Cs[x,y] = sim\n",
    "\n",
    "dic = {}\n",
    "for _ in range(len(phonemes)):\n",
    "    C, C_kmeans = np.unravel_index(sims_Cs.argmax(), sims_Cs.shape)\n",
    "    dic[phonemes[C]] = C_kmeans\n",
    "    overlaps[C_idx,:] = np.zeros((1,len(phoenemes)))\n",
    "    overlaps[:,phoneme_idx] = np.zeros((len(phoenemes),1))\n",
    "\n",
    "print(matches / len(phonemes))\n",
    "print(\"Random would be 1 <3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d277a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlaps = np.zeros((len(phonemes), len(phonemes))) # (Cs x phonemes)\n",
    "\n",
    "for C_idx, assignment in enumerate(assignments):\n",
    "    for idx in assignment:\n",
    "        overlaps[C_idx, phonemes.index(labs[idx])] += 1\n",
    "\n",
    "dic = {}\n",
    "\n",
    "for _ in range(len(phonemes)):\n",
    "    C_idx, phoneme_idx = np.unravel_index(overlaps.argmax(), overlaps.shape)\n",
    "    dic[phonemes[phoneme_idx]] = C_idx\n",
    "    overlaps[C_idx,:] = np.zeros((1,len(phonemes)))\n",
    "    overlaps[:,phoneme_idx] = np.zeros((len(phonemes),))\n",
    "\n",
    "Cs_final = []\n",
    "Ns_final = []\n",
    "for phoneme in [p for p in phonemes if p in list(dic.keys()) ]:\n",
    "    Cs_final.append(Cs_kmeans[dic[phoneme]])\n",
    "    Ns_final.append(Ns_kmeans[dic[phoneme]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c8c7e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-from output\n",
      "---- success\n",
      "Accuracy:  0.17823529411764705\n",
      "n_mels:  5\n",
      "d:  False\n",
      "dd:  False\n",
      "speakers:  []\n",
      "dr:  []\n",
      "dr:  1100\n",
      "Test:  Test\n",
      "subsamples:  5\n",
      "Neurons:  10\n",
      "Aperture:  5\n"
     ]
    }
   ],
   "source": [
    "#------ Custom test\n",
    "correct = 0\n",
    "n=0\n",
    "fv, lv, _ = fc.collectFeaturesInSegments(\n",
    "    ft='Test',n_mels=n_mels,delta=delta,delta_delta=delta_delta,\n",
    "    long_version=long_version,speakers=speakers,dr=dr,subsamples=subsamples,\n",
    "    path_option=path_option+\"_test\")\n",
    "\n",
    "test_group = {}\n",
    "for key in group.keys():\n",
    "    test_group[key] = []\n",
    "\n",
    "for i in range(len(fv)):\n",
    "    if lv[i] in group.keys():\n",
    "        test_group[lv[i]].append(fv[i])\n",
    "\n",
    "for i,(x,y) in enumerate(test_group.items()):\n",
    "    for sample in y[:200]:\n",
    "        z = esn.run(sample.T)\n",
    "        es = evidences_for_Cs_z(z,Cs_final,Ns_final)\n",
    "        es = [ np.sum(p) for p in es ]\n",
    "        correct += x==list(test_group.keys())[np.argmax(es)]\n",
    "        n+=1\n",
    "        #print(list(group.keys())[np.argmax(es)])\n",
    "\n",
    "print(\"Accuracy: \",correct/n)\n",
    "print(\"n_mels: \",n_mels)\n",
    "print(\"d: \",delta)\n",
    "print(\"dd: \",delta_delta)\n",
    "print(\"speakers: \",speakers)\n",
    "print(\"dr: \",dr)\n",
    "print(\"dr: \",min_samples)\n",
    "print(\"Test: \",\"Test\")\n",
    "print(\"subsamples: \",subsamples)\n",
    "print(\"Neurons: \",esn_params[\"N\"])\n",
    "print(\"Aperture: \",aperture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d70c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2]",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
