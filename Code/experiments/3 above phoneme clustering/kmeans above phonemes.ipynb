{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# KMeans Above Phonemes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Collect training features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-from output\n",
      "---- success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "247"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import random\n",
    "random.seed(0)\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "directory = os.path.abspath('/Users/joris/Documents/Work/bsc ai/bt/Bachelor-Thesis/code')\n",
    "sys.path.append(directory)\n",
    "\n",
    "from lib.conceptors import *\n",
    "from lib.esn import ESN\n",
    "from lib.helpers import *\n",
    "from dataset.loading import Feature_Collector\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "XorZ = \"X\"\n",
    "\n",
    "print(\"starting\")\n",
    "\n",
    "path = '../../data/'\n",
    "fc = Feature_Collector(path)\n",
    "\n",
    "dr = []\n",
    "speakers = []\n",
    "\n",
    "long_version = False\n",
    "n_mels = 13\n",
    "delta = False\n",
    "delta_delta = False\n",
    "subsamples = 10\n",
    "\n",
    "path_option = \"Final\"+str(long_version)+str(n_mels)+str(delta)+str(delta_delta)+str(subsamples)\n",
    "\n",
    "if dr:\n",
    "    path_option = str(dr)+\"_\"+path_option\n",
    "if len(speakers):\n",
    "    path_option = str(speakers[0])+\"_\"+path_option\n",
    "\n",
    "features_train, labels_train, _ = fc.collectFeaturesInSegments(\n",
    "    n_mels=n_mels, delta=delta, delta_delta=delta_delta,\n",
    "    long_version=long_version, speakers=speakers, dr=dr,\n",
    "    subsamples=subsamples, path_option=path_option)\n",
    "\n",
    "print(f\"Loaded to {len(features_train)} samples of shape {features_train[0].shape}\")\n",
    "\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15033d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init reservoir\n",
    "import random\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "t_washout = 0 # number of washout steps\n",
    "aperture = 5\n",
    "nb_trials = 1\n",
    "\n",
    "esn_params = {\n",
    "    \"in_dim\": n_mels*(1+delta+delta_delta),\n",
    "    \"out_dim\": n_mels*(1+delta+delta_delta),\n",
    "    \"N\": 10,\n",
    "    \"W_in_scale\": 1.5,\n",
    "    \"b_scale\": .2,\n",
    "    \"spectral_radius\": 1.5\n",
    "}\n",
    "\n",
    "esn = ESN(esn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b75a23c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "18 phonemes\n"
     ]
    }
   ],
   "source": [
    "group = {}\n",
    "for i in range(len(features)):\n",
    "    if labels[i] not in group.keys():\n",
    "        group[labels[i]] = []\n",
    "    group[labels[i]].append(features[i])\n",
    "group2 = {}\n",
    "# Cut down to same number of samples\n",
    "min_samples = 10\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "for x,y in group.items():\n",
    "    if x in ['ih', 'ah', 'ae', 'iy', 'aa', 'eh', 'ey', 'ay', 'uh', 'oy'] + ['t', 'b', 'd', 'p', 'dx', 'k', 'dh', 'g']:\n",
    "        group2[x] = random.sample(y, min_samples)\n",
    "        data += group2[x]\n",
    "        labels += [x]*min_samples\n",
    "group = group2\n",
    "\n",
    "phonemes = list(group.keys())\n",
    "print(min_samples)\n",
    "print(str(len(phonemes))+\" phonemes\")\n",
    "#[print(x,\" ss \",len(y)) for x,y in group.items() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58444d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "(90, 4, 10)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 4 is different from 10)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [22]\u001B[0m, in \u001B[0;36m<cell line: 53>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Cs, new_assignments\n\u001B[0;32m     52\u001B[0m \u001B[38;5;66;03m#######################################\u001B[39;00m\n\u001B[1;32m---> 53\u001B[0m Cs_kmeans, assignments_kmeans \u001B[38;5;241m=\u001B[39m \u001B[43mkmeans\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnb_conceptors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maperture\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maperture\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [22]\u001B[0m, in \u001B[0;36mkmeans\u001B[1;34m(data, nb_conceptors, aperture, max_epochs)\u001B[0m\n\u001B[0;32m     19\u001B[0m X \u001B[38;5;241m=\u001B[39m data[new_assignments[\u001B[38;5;241m0\u001B[39m],:,:]\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(X\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m---> 21\u001B[0m Cs \u001B[38;5;241m=\u001B[39m [ compute_c(data[assignments], aperture) \u001B[38;5;28;01mfor\u001B[39;00m assignments \u001B[38;5;129;01min\u001B[39;00m new_assignments ]\n\u001B[0;32m     22\u001B[0m Ns \u001B[38;5;241m=\u001B[39m Ns_from_Cs(Cs)\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m#print(\"- optimizing +\")\u001B[39;00m\n",
      "Input \u001B[1;32mIn [22]\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     19\u001B[0m X \u001B[38;5;241m=\u001B[39m data[new_assignments[\u001B[38;5;241m0\u001B[39m],:,:]\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(X\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m---> 21\u001B[0m Cs \u001B[38;5;241m=\u001B[39m [ \u001B[43mcompute_c\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[43massignments\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maperture\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m assignments \u001B[38;5;129;01min\u001B[39;00m new_assignments ]\n\u001B[0;32m     22\u001B[0m Ns \u001B[38;5;241m=\u001B[39m Ns_from_Cs(Cs)\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m#print(\"- optimizing +\")\u001B[39;00m\n",
      "File \u001B[1;32mc:\\main\\Work\\thesis\\Bachelor-Thesis\\Code\\lib\\conceptors.py:25\u001B[0m, in \u001B[0;36mcompute_c\u001B[1;34m(X, aperture, weights)\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_c\u001B[39m(X, aperture, weights\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m     21\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;124;03m    Computes conceptor. Potentially using via a weighted correlation matrix.\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;124;03m    param :Weights: weights for each time step (column) in X\u001B[39;00m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 25\u001B[0m     R \u001B[38;5;241m=\u001B[39m \u001B[43mcorr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;66;03m# Eig_vals, Eig_vecs = np.linalg.eig(R)\u001B[39;00m\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;66;03m# U = Eig_vecs\u001B[39;00m\n\u001B[0;32m     28\u001B[0m     \u001B[38;5;66;03m# Sigma = np.diag(Eig_vals)\u001B[39;00m\n\u001B[0;32m     29\u001B[0m     \u001B[38;5;66;03m# for elem in Eig_vals:\u001B[39;00m\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;66;03m#     if abs(elem) < 1e-100:\u001B[39;00m\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;66;03m#         print(\"!!! Zero singular value(s) !!!\")\u001B[39;00m\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m R \u001B[38;5;241m@\u001B[39m inv( R \u001B[38;5;241m+\u001B[39m aperture \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m (\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39meye(R\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]))\n",
      "File \u001B[1;32mc:\\main\\Work\\thesis\\Bachelor-Thesis\\Code\\lib\\conceptors.py:18\u001B[0m, in \u001B[0;36mcorr\u001B[1;34m(X, weights)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     17\u001B[0m     norm \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m1\u001B[39m \u001B[38;5;66;03m# L\u001B[39;00m\n\u001B[1;32m---> 18\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mX\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m@\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m \u001B[38;5;241m/\u001B[39m norm\n",
      "\u001B[1;31mValueError\u001B[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 4 is different from 10)"
     ]
    }
   ],
   "source": [
    "def assign_to_clusters(nb_points, nb_clusters):\n",
    "    assignments = [ [] for _ in range(nb_clusters) ]\n",
    "    points = [ x for x in range(nb_points) ]\n",
    "    np.random.shuffle(points)\n",
    "    for i in range(nb_clusters):\n",
    "        assignments[i] = points[i*int(nb_points/nb_clusters):(i+1)*int(nb_points/nb_clusters)]\n",
    "    return assignments\n",
    "\n",
    "# K-means\n",
    "def kmeans(data, nb_conceptors, aperture, max_epochs=100):\n",
    "    # Initial assignments and initial conceptors\n",
    "    nb_points = data.shape[0]\n",
    "    new_assignments = assign_to_clusters(nb_points, nb_conceptors)\n",
    "    # Training loop\n",
    "    for epoch in range(max_epochs):\n",
    "        print(\"epoch:\",epoch)\n",
    "        # recompute centroids based on subset of assigned state\n",
    "        Cs = [ [] for _ in range(nb_conceptors) ]\n",
    "        for i, assignments in enumerate(new_assignments):\n",
    "            Z = np.array([])\n",
    "            for x in assignments:\n",
    "                z = esn.run(data[x].T)\n",
    "                Z = np.hstack([Z, z]) if Z.size else z\n",
    "            Cs[i] = compute_c(Z, aperture)\n",
    "        Ns = Ns_from_Cs(Cs)\n",
    "        print(\"- optimizing +\")\n",
    "        Cs = optimize_apertures(Cs)\n",
    "        print(\"- optimizing -\")\n",
    "        Ns = optimize_apertures(Ns)\n",
    "        \n",
    "        # recompute assignments by find the closest conceptor for each of the state points\n",
    "        old_assignments = new_assignments.copy()\n",
    "        new_assignments = [ [] for _ in range(nb_conceptors) ]\n",
    "        \n",
    "        Es = np.zeros((nb_conceptors, nb_points))\n",
    "        for t in range(nb_points):\n",
    "            Es[:,t] = np.array( evidences_for_Cs_z(X[:,t], Cs, Ns) )\n",
    "        \n",
    "        for t in range(nb_points):\n",
    "            conceptor_index = np.argmax(Es[:,t])\n",
    "            new_assignments[ conceptor_index ].append(t)\n",
    "\n",
    "        # stop if converged\n",
    "        for new_assignment, old_assignment in zip(new_assignments, old_assignments):\n",
    "            if set(new_assignment) == set(old_assignment):\n",
    "                print(\"Converged\")\n",
    "                return Cs, new_assignments\n",
    "\n",
    "    return Cs, new_assignments\n",
    "\n",
    "#######################################\n",
    "Cs_kmeans, assignments_kmeans = kmeans(data, nb_conceptors=2, aperture=aperture, max_epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6179483b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2]",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
