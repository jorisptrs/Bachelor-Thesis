{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60b2514c",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55fb8b6",
   "metadata": {},
   "source": [
    "### Collect training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74e86d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "-from output\n",
      "---- success\n"
     ]
    },
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import random\n",
    "random.seed(0)\n",
    "import os, sys\n",
    "\n",
    "directory = os.path.abspath('/Users/joris/Documents/Work/bsc ai/thesis/bachelor-thesis/code/')\n",
    "sys.path.append(directory)\n",
    "\n",
    "from data.loading import Feature_Collector\n",
    "from lib.conceptors import *\n",
    "from lib.esn import ESN\n",
    "from lib.helpers import *\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "XorZ = \"X\"\n",
    "#method = \"centroids\"\n",
    "#method = \"sims\"\n",
    "method = \"pred\"\n",
    "#method = \"ogsignals\"\n",
    "\n",
    "print(\"starting\")\n",
    "\n",
    "path = '../timit/'\n",
    "fc = Feature_Collector(path)\n",
    "\n",
    "dr = []\n",
    "speakers = []\n",
    "\n",
    "long_version = False\n",
    "n_mels = 13\n",
    "delta = False\n",
    "delta_delta = False\n",
    "subsamples = 10\n",
    "\n",
    "path_option = \"Final\"+str(long_version)+str(n_mels)+str(delta)+str(delta_delta)+str(subsamples)\n",
    "\n",
    "if dr:\n",
    "    path_option = str(dr)+\"_\"+path_option\n",
    "if len(speakers):\n",
    "    path_option = str(speakers[0])+\"_\"+path_option\n",
    "\n",
    "features,labels,_ = fc.collectFeaturesInSegments(\n",
    "    n_mels=n_mels,delta=delta,delta_delta=delta_delta,\n",
    "    long_version=long_version,speakers=speakers,dr=dr,\n",
    "    subsamples=subsamples,path_option=path_option)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cebfd6",
   "metadata": {},
   "source": [
    "### Regroup data and subset phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d56f220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to 390 samples of shape (10, 13)\n"
     ]
    }
   ],
   "source": [
    "def filter_data(features, labels, classes=None,trunkate=False,limit=1000000):\n",
    "    group = {}\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        if classes != None and labels[i] not in classes:\n",
    "            continue\n",
    "        if labels[i] not in group.keys():\n",
    "            group[labels[i]] = []\n",
    "        if len(group[labels[i]]) < limit:\n",
    "            group[labels[i]].append(features[i])\n",
    "\n",
    "    samples_per_phoneme = min([ len(value) for value in group.values() ])\n",
    "    if trunkate:\n",
    "        for key in group.keys():\n",
    "            group[key] = random.sample(group[key], samples_per_phoneme)\n",
    "\n",
    "    classes = list(group.keys())\n",
    "    filtered_labels = []\n",
    "    filtered_features = []\n",
    "    for label, features in group.items():\n",
    "        for feature in features:\n",
    "            filtered_features.append(feature)\n",
    "            filtered_labels.append(label)\n",
    "    return classes, filtered_features, filtered_labels\n",
    "\n",
    "classes, features, labels = filter_data(features, labels, classes=None, trunkate=False, limit=10)\n",
    "\n",
    "print(f\"Filtered to {len(features)} samples of shape {features[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390a2ec9",
   "metadata": {},
   "source": [
    "### Compute conceptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b268477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Cs_and_Ns(group, esn, aperture):\n",
    "    Cs_clas = []\n",
    "    for phoneme, signals in group.items():\n",
    "        X = np.array([])\n",
    "        for signal in signals:\n",
    "            x = esn.run(signal.T, XorZ=XorZ)\n",
    "            X = np.hstack((X, x)) if X.size else x\n",
    "        Cs_clas.append(compute_c(X, aperture))\n",
    "    print(\"optimizing\")\n",
    "    Cs_clas = optimize_apertures(Cs_clas)\n",
    "    print(\"normalizing\")\n",
    "    Cs_clas = normalize_apertures(Cs_clas)\n",
    "    print(\"- computing negative conceptors\")\n",
    "    Ns_clas = Ns_from_Cs(Cs_clas)\n",
    "    #Ns_clas = optimize_apertures(Ns_clas)\n",
    "    #Ns_clas = normalize_apertures(Ns_clas)\n",
    "    return Cs_clas, Ns_clas\n",
    "\n",
    "def compute_X_centroids(group, esn):\n",
    "    centroids = []\n",
    "    for phoneme, signals in group.items():\n",
    "        reservoir_states = []\n",
    "        for signal in signals:\n",
    "            x = esn.run(signal.T, XorZ=XorZ)\n",
    "            reservoir_states.append(x)\n",
    "        reservoir_states = np.array(reservoir_states)\n",
    "        centroids.append(np.mean(reservoir_states,axis=0))\n",
    "    return centroids\n",
    "\n",
    "def original_signal_centroids(group):\n",
    "    centroids = []\n",
    "    for phoneme, signals in group.items():\n",
    "        signal_array = np.array(signals)\n",
    "        centroids.append(np.mean(signal_array,axis=0))\n",
    "    return centroids\n",
    "\n",
    "def d(x,y):\n",
    "    return np.linalg.norm(x-y)\n",
    "\n",
    "# only optimize Cs before : g=133.3 a=0.5516\n",
    "# optimize Cs and Ns after : g1=133.3 g2=.5 a=.\n",
    "# only normalize Cs before : \n",
    "# normalize Cs and Ns after : \n",
    "# normalize Cs and Ns before and after : \n",
    "# similarities : a=0.2543922127255461"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70fbab4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class Classifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, W_in_scale=1.1, spectral_radius=2.57, b_scale=.44, weights=.1):\n",
    "        self.W_in_scale = W_in_scale\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.b_scale = b_scale\n",
    "        self.weights = weights\n",
    "\n",
    "    def fit(self, X, y, **params):\n",
    "        # Store the classes seen during fit\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        \n",
    "        # Group data by class\n",
    "        group = {}\n",
    "        for i in range(len(y)):\n",
    "            if self.y_[i] not in group.keys():\n",
    "                group[self.y_[i]] = []\n",
    "            group[self.y_[i]].append(self.X_[i])\n",
    "        self.classes = list(group.keys())\n",
    "        samples_per_phoneme = min([ len(value) for value in group.values() ])\n",
    "        self.n_samples = sum([len(x) for x in list(group.values())])\n",
    "        print(\"Number of samples:\", self.n_samples)\n",
    "        print(self.classes)\n",
    "        \n",
    "        # Init Reservoir\n",
    "        esn_params = {\n",
    "            \"in_dim\": params[\"in_dim\"],\n",
    "            \"out_dim\": params[\"out_dim\"],\n",
    "            \"N\": 40+60*(XorZ==\"X\"),\n",
    "            \"W_in_scale\": self.W_in_scale,\n",
    "            \"b_scale\": self.b_scale,\n",
    "            \"spectral_radius\": self.spectral_radius,\n",
    "            \"weights\": self.weights\n",
    "        }\n",
    "\n",
    "        self.esn = ESN(esn_params)\n",
    "        if method == \"pred\" or method == \"sims\":\n",
    "            self.Cs_clas, self.Ns_clas = compute_Cs_and_Ns(group, esn=self.esn, aperture=1)\n",
    "        elif method == \"centroids\":\n",
    "            self.centroids = compute_X_centroids(group, esn=self.esn)\n",
    "        else:\n",
    "            self.centroids = original_signal_centroids(group)\n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        y = []\n",
    "        for sample in X:\n",
    "            if method == \"sims\":\n",
    "                x = self.esn.run(sample.T, XorZ=XorZ)\n",
    "                C_loc = compute_c(x,aperture=133.3)\n",
    "                es = [ similarity_c(C,C_loc) for C in self.Cs_clas ]\n",
    "                es = [ np.sum(p) for p in es ]\n",
    "                y.append(self.classes[np.argmax(es)])\n",
    "            elif method == \"centroids\":\n",
    "                x = self.esn.run(sample.T, XorZ=XorZ)\n",
    "                es = [ d(x,centroid) for centroid in self.centroids ]\n",
    "                y.append(self.classes[np.argmin(es)])\n",
    "            elif method == \"ogsignals\":\n",
    "                es = [ d(sample,centroid) for centroid in self.centroids ]\n",
    "                y.append(self.classes[np.argmin(es)])\n",
    "            else:\n",
    "                x = self.esn.run(sample.T, XorZ=XorZ)\n",
    "                es = evidences_for_Cs(x,self.Cs_clas,self.Ns_clas)\n",
    "                if XorZ == \"X\":\n",
    "                    es = [ np.sum(p) for p in es ]\n",
    "                y.append(self.classes[np.argmax(es)])\n",
    "                \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d29a0e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 20865\n",
      "['h#', 'w', 'ih', 's', 'ah', 'ch', 'n', 'ae', 't', 'v', 'r', 'f', 'y', 'uw', 'sh', 'l', 'b', 'iy', 'aa', 'd', 'eh', 'p', 'z', 'ey', 'dx', 'ay', 'ng', 'k', 'dh', 'er', 'm', 'jh', 'g', 'ow', 'aw', 'hh', 'uh', 'oy', 'th']\n",
      "optimizing\n",
      "normalizing\n",
      "- computing negative conceptors\n",
      "Computing gammas...\n",
      "1  of  39\n",
      "2  of  39\n",
      "3  of  39\n",
      "4  of  39\n",
      "5  of  39\n",
      "6  of  39\n",
      "7  of  39\n",
      "8  of  39\n",
      "9  of  39\n",
      "10  of  39\n",
      "11  of  39\n",
      "12  of  39\n",
      "13  of  39\n",
      "14  of  39\n",
      "15  of  39\n",
      "16  of  39\n",
      "17  of  39\n",
      "18  of  39\n",
      "19  of  39\n",
      "20  of  39\n",
      "21  of  39\n",
      "22  of  39\n",
      "23  of  39\n",
      "24  of  39\n",
      "25  of  39\n",
      "26  of  39\n",
      "27  of  39\n",
      "28  of  39\n",
      "29  of  39\n",
      "30  of  39\n",
      "31  of  39\n",
      "32  of  39\n",
      "33  of  39\n",
      "34  of  39\n",
      "35  of  39\n",
      "36  of  39\n",
      "37  of  39\n",
      "38  of  39\n",
      "39  of  39\n",
      "Optimal gamma:  121.84829059829059\n",
      "Target:  48.5602879171468\n",
      "std 1.165561349120131\n",
      "0.757790696431628\n",
      "0.49090597320785684\n",
      "0.3172331346766788\n",
      "0.20467880234225605\n",
      "0.1319327003672437\n",
      "0.08499640470106615\n",
      "0.05474415163552656\n",
      "0.03525672972193866\n",
      "0.022707197140398008\n",
      "0.014626292158130882\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Classifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Classifier</label><div class=\"sk-toggleable__content\"><pre>Classifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Classifier()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'W_in_scale' : Real(0.05,2),\n",
    "    'spectral_radius' : Real(0.05,4),\n",
    "    'b_scale' : Real(0,2),\n",
    "    'weights' : Real(0.05,0.95)\n",
    "}\n",
    "\n",
    "#opt = Classifier()\n",
    "\n",
    "#opt = Classifier(1.5,1.5,.2,.1) # Z optimal params\n",
    "opt = Classifier(1.1,2.57,.44,.1) # X optimal params\n",
    "#opt = BayesSearchCV(Classifier(), parameters, n_iter=50, cv=3)\n",
    "opt.fit(features, labels, **{\n",
    "    \"in_dim\":n_mels,\n",
    "    \"out_dim\":n_mels\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e330ce",
   "metadata": {},
   "source": [
    "## Testing\n",
    "### Feature Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59988fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "-from output\n",
      "---- success\n",
      "64145\n",
      "0.47946343779677114\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing...\")\n",
    "\n",
    "fv, lv, _ = fc.collectFeaturesInSegments(\n",
    "    ft='Test',n_mels=n_mels,delta=delta,delta_delta=delta_delta,\n",
    "    long_version=long_version,speakers=[],dr=dr,sentence=[],\n",
    "    subsamples=subsamples,path_option=path_option+\"_test\")\n",
    "\n",
    "print(len(fv))\n",
    "classes, fv, lv = filter_data(fv, lv, classes=None,trunkate=True,limit=1000000)\n",
    "\n",
    "print(opt.score(fv, lv))\n",
    "#print(opt.best_params_)\n",
    "\n",
    "#with open('res.txt','x') as file:\n",
    "#    file.write(opt.score(fv,lv))\n",
    "#    file.write(opt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1afa19",
   "metadata": {},
   "source": [
    "# Disjunction method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e7f07c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Cs_and_Ns_below_phoneme(features, labels, esn, aperture):\n",
    "    all_Cs = []\n",
    "    Cs = {}\n",
    "    for signal, label in zip(features, labels):\n",
    "        x = esn.run(signal.T)\n",
    "        C = compute_c(x, aperture)\n",
    "        if label not in Cs.keys():\n",
    "            Cs[label] = C\n",
    "        else:\n",
    "            Cs[label] = OR_C(Cs[label], C)\n",
    "    #phonemes = list(Cs.keys())\n",
    "    Cs = list(Cs.values())\n",
    "    print(\"- optimizing +\")\n",
    "    Cs = optimize_apertures(Cs)\n",
    "    Cs = normalize_apertures(Cs)\n",
    "    print(\"- computing negative conceptors\")\n",
    "    Ns = Ns_from_Cs(Cs)\n",
    "    return Cs, Ns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
